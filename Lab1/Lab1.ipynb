{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0cd92519e450c25804136c2008eef24c3d70be2e96d599be5f96d22ea88f45fe2",
   "display_name": "Python 3.9.4 64-bit ('pydsEnv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**Lab1**\n",
    "Carlos Garc√≠a 21000475"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.5.0-rc3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Enabled compatitility to tf1.x\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  print(\"Enabled compatitility to tf1.x\")"
   ]
  },
  {
   "source": [
    "### Getting the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('inputs/proyecto_training_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1460, 6)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "n_train = int(np.ceil(data.shape[0]*0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data[:n_train, :], data[n_train:, :] \n",
    "assert (train.shape[0] + test.shape[0]) == data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   SalePrice  OverallQual  1stFlrSF  TotRmsAbvGrd  YearBuilt  LotFrontage\n",
       "0   208500.0          7.0     856.0           8.0     2003.0         65.0\n",
       "1   181500.0          6.0    1262.0           6.0     1976.0         80.0\n",
       "2   223500.0          7.0     920.0           6.0     2001.0         68.0\n",
       "3   140000.0          7.0     961.0           7.0     1915.0         60.0\n",
       "4   250000.0          8.0    1145.0           9.0     2000.0         84.0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SalePrice</th>\n      <th>OverallQual</th>\n      <th>1stFlrSF</th>\n      <th>TotRmsAbvGrd</th>\n      <th>YearBuilt</th>\n      <th>LotFrontage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>208500.0</td>\n      <td>7.0</td>\n      <td>856.0</td>\n      <td>8.0</td>\n      <td>2003.0</td>\n      <td>65.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>181500.0</td>\n      <td>6.0</td>\n      <td>1262.0</td>\n      <td>6.0</td>\n      <td>1976.0</td>\n      <td>80.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>223500.0</td>\n      <td>7.0</td>\n      <td>920.0</td>\n      <td>6.0</td>\n      <td>2001.0</td>\n      <td>68.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>140000.0</td>\n      <td>7.0</td>\n      <td>961.0</td>\n      <td>7.0</td>\n      <td>1915.0</td>\n      <td>60.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>250000.0</td>\n      <td>8.0</td>\n      <td>1145.0</td>\n      <td>9.0</td>\n      <td>2000.0</td>\n      <td>84.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ds_train = pd.DataFrame(train, columns = ['SalePrice', 'OverallQual', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt', 'LotFrontage'])\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'SalePrice'\n",
    "regressor = 'OverallQual'"
   ]
  },
  {
   "source": [
    "### Defining the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(y_real,y_aprox):\n",
    "  return 1/2 * tf.reduce_mean(tf.math.square(y_real - y_aprox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mini batch model\n",
    "def trainModel(x, y, epochs = 100, batch_size = 10, lr = 0.001, kprint = 10):\n",
    "    \n",
    "    #define iterations\n",
    "    total_iterations = x.shape[0] // batch_size\n",
    "\n",
    "    #initializing the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    #initializing data\n",
    "    tensor_x = tf.placeholder(tf.float32, [None, 2], \"tensor_x\")\n",
    "    tensor_y = tf.placeholder(tf.float32, [None, 1], \"tensor_y\")\n",
    "    w = tf.get_variable(\"parameters_m_b\", dtype=tf.float32, shape=[2,1],\n",
    "                    initializer=tf.zeros_initializer())\n",
    "\n",
    "    #estimating values\n",
    "    yhat = tf.matmul(tensor_x, w, name = 'yhat')\n",
    "\n",
    "    #Cost/Error calculation\n",
    "    with tf.name_scope(\"cost_definition\"):\n",
    "        cost = error(tensor_y, yhat)\n",
    "        \n",
    "    #Scalar summary\n",
    "    cost_summary = tf.summary.scalar(name = 'MSE', tensor = cost)\n",
    "\n",
    "    #gradients and cost/error optimization\n",
    "    with tf.name_scope(\"params_update\"):\n",
    "        gradients = tf.gradients(cost, [w], name = 'gradients') #calculating error and gradients\n",
    "        w_update = tf.assign(w, w - lr * gradients[0], name = 'weigths_update') #updating parameters weights\n",
    "    \n",
    "    #with tf.train.MonitoredSession() as session: #this object does'nt generate a clean graph due to initialized variables\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        #Reshaping the data\n",
    "        x = x[:, np.newaxis]\n",
    "        x = np.hstack((x, np.ones_like(x)))\n",
    "        y = y[:, np.newaxis]\n",
    "\n",
    "        feed_dict_model = {tensor_x: x, tensor_y: y} #whole batch dictionary\n",
    "\n",
    "        #Define tensorboard writer and config string\n",
    "        dt_string = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        writer = tf.summary.FileWriter('./graphs/{}_lm_epochs={}_mbatch={}_lr={}'.format(dt_string, epochs, batch_size, lr), session.graph)\n",
    "        \n",
    "        for epoch in range(0, epochs):    \n",
    "            for i in range(0, total_iterations):\n",
    "                start_sample = i * batch_size\n",
    "                end_sample = start_sample + batch_size\n",
    "                x_mb = x[start_sample:end_sample]\n",
    "                y_mb = y[start_sample:end_sample]\n",
    "                \n",
    "                feed_dict = {tensor_x: x_mb, tensor_y: y_mb}\n",
    "                _, weights = session.run([w_update, w], feed_dict = feed_dict) #be careful not to use the same name of a previous variable\n",
    "                #m, b = weights[0, 0], weights[1, 0]\n",
    "\n",
    "                #print(\"Epoch {} iteration {} m={} b={}\".format(epoch,i,m,b))\n",
    "\n",
    "            predictions = session.run(yhat, feed_dict = feed_dict_model)\n",
    "            mse, csummary = session.run([cost, cost_summary], feed_dict = feed_dict_model)\n",
    "            writer.add_summary(csummary, epoch + 1)\n",
    "\n",
    "            if (epoch + 1) % kprint == 0:            \n",
    "                m, b = weights[0, 0], weights[1, 0]\n",
    "                print(\"Epoch {} parameters: m={} b={} mse={}\".format(epoch + 1, m, b, mse))\n",
    "\n",
    "        _, weights = session.run([w_update, w], feed_dict = feed_dict_model)\n",
    "        predictions, mse = session.run([yhat, cost], feed_dict = feed_dict_model)\n",
    "        m, b = weights[0, 0], weights[1, 0]\n",
    "        print(\"Final model parameters: m={} b={} mse={}\".format(m,b,mse))\n",
    "\n",
    "        writer.close()\n",
    "            "
   ]
  },
  {
   "source": [
    "### Graph definition\n",
    "\n",
    "<img src=\"imgs/graph_definition.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 parameters: m=30116.4921875 b=2648.34814453125 mse=1385676160.0\n",
      "Epoch 20 parameters: m=30420.65625 b=697.010986328125 mse=1376387200.0\n",
      "Epoch 30 parameters: m=30719.11328125 b=-1217.7012939453125 mse=1367451648.0\n",
      "Epoch 40 parameters: m=31011.96875 b=-3096.478271484375 mse=1358856704.0\n",
      "Epoch 50 parameters: m=31299.330078125 b=-4939.99365234375 mse=1350589696.0\n",
      "Epoch 60 parameters: m=31581.294921875 b=-6748.91015625 mse=1342637952.0\n",
      "Epoch 70 parameters: m=31857.96875 b=-8523.8759765625 mse=1334989568.0\n",
      "Epoch 80 parameters: m=32129.44921875 b=-10265.53125 mse=1327633536.0\n",
      "Epoch 90 parameters: m=32395.8359375 b=-11974.498046875 mse=1320558080.0\n",
      "Epoch 100 parameters: m=32657.22265625 b=-13651.390625 mse=1313753472.0\n",
      "Final model parameters: m=32594.11328125 b=-13701.400390625 mse=1313203584.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment1. Initial experiment, expecting MSE going down. This experiment will work as a starting point to tune next experiments. Small lr and epochs\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 100, batch_size = 250, lr = 0.01, kprint = 10 )"
   ]
  },
  {
   "source": [
    "Good starting point, MSE is going down at a good peace but too few epochs implemented"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=32657.22265625 b=-13651.390625 mse=1313753472.0\n",
      "Epoch 200 parameters: m=35015.90625 b=-28783.232421875 mse=1258518272.0\n",
      "Epoch 300 parameters: m=36967.48046875 b=-41303.3125 mse=1221211904.0\n",
      "Epoch 400 parameters: m=38582.2109375 b=-51662.41796875 mse=1196092416.0\n",
      "Epoch 500 parameters: m=39918.2421875 b=-60233.546875 mse=1179243264.0\n",
      "Epoch 600 parameters: m=41023.671875 b=-67325.3046875 mse=1167995648.0\n",
      "Epoch 700 parameters: m=41938.30859375 b=-73193.03125 mse=1160533888.0\n",
      "Epoch 800 parameters: m=42695.07421875 b=-78047.9765625 mse=1155622016.0\n",
      "Epoch 900 parameters: m=43321.234375 b=-82065.0078125 mse=1152422272.0\n",
      "Epoch 1000 parameters: m=43839.3046875 b=-85388.6328125 mse=1150366592.0\n",
      "Final model parameters: m=43778.48828125 b=-85403.4921875 mse=1150052608.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment2. Since the MSE decayed really quickly will only add more epochs. This will keep lowering it down\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 250, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "As expected initial hyper parameters work fine, increasing epochs shows a good progress"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=31320.9453125 b=-4932.8759765625 mse=1350775296.0\n",
      "Epoch 200 parameters: m=32675.421875 b=-13640.44921875 mse=1313930752.0\n",
      "Epoch 300 parameters: m=33907.5234375 b=-21561.31640625 mse=1283609856.0\n",
      "Epoch 400 parameters: m=35028.3125 b=-28766.544921875 mse=1258671360.0\n",
      "Epoch 500 parameters: m=36047.83984375 b=-35320.82421875 mse=1238173184.0\n",
      "Epoch 600 parameters: m=36975.25390625 b=-41282.921875 mse=1221337216.0\n",
      "Epoch 700 parameters: m=37818.875 b=-46706.34375 mse=1207519616.0\n",
      "Epoch 800 parameters: m=38586.28125 b=-51639.78125 mse=1196189696.0\n",
      "Epoch 900 parameters: m=39284.35546875 b=-56127.52734375 mse=1186908800.0\n",
      "Epoch 1000 parameters: m=39919.359375 b=-60209.7890625 mse=1179315200.0\n",
      "Final model parameters: m=39855.8515625 b=-60237.28125 mse=1178926208.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 3. Experiment with a higher batch size may cause a better performance, so batch_size is increasing to 500\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 500, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too high batch size is not working since MSE is going up again"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=33271.0859375 b=-17741.33203125 mse=1297591040.0\n",
      "Epoch 200 parameters: m=36031.0546875 b=-35391.578125 mse=1237790976.0\n",
      "Epoch 300 parameters: m=38208.921875 b=-49319.2265625 mse=1201270144.0\n",
      "Epoch 400 parameters: m=39927.4609375 b=-60309.42578125 mse=1179094144.0\n",
      "Epoch 500 parameters: m=41283.55078125 b=-68981.7421875 mse=1165730432.0\n",
      "Epoch 600 parameters: m=42353.6484375 b=-75825.1015625 mse=1157760128.0\n",
      "Epoch 700 parameters: m=43198.046875 b=-81225.109375 mse=1153074944.0\n",
      "Epoch 800 parameters: m=43864.35546875 b=-85486.2109375 mse=1150376064.0\n",
      "Epoch 900 parameters: m=44390.12890625 b=-88848.5625 mse=1148868224.0\n",
      "Epoch 1000 parameters: m=44805.015625 b=-91501.796875 mse=1148065536.0\n",
      "Final model parameters: m=44739.3828125 b=-91514.4453125 mse=1147707904.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 4. Higher batchsize didn't work out, lowering it down a little more to check for optimal hyper parameters, expect better performance\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 200, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "Lowering batch size a bit more makes an improvement without consuming too much additional time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=39151.5234375 b=-63683.34765625 mse=1198726784.0\n",
      "Epoch 200 parameters: m=42793.3515625 b=-86464.453125 mse=1171249792.0\n",
      "Epoch 300 parameters: m=44011.484375 b=-94084.390625 mse=1167746048.0\n",
      "Epoch 400 parameters: m=44418.96484375 b=-96633.3515625 mse=1167209984.0\n",
      "Epoch 500 parameters: m=44555.234375 b=-97485.78125 mse=1167102080.0\n",
      "Epoch 600 parameters: m=44600.8203125 b=-97770.9140625 mse=1167074304.0\n",
      "Epoch 700 parameters: m=44616.05859375 b=-97866.2578125 mse=1167065600.0\n",
      "Epoch 800 parameters: m=44621.203125 b=-97898.4296875 mse=1167062784.0\n",
      "Epoch 900 parameters: m=44622.78515625 b=-97908.3046875 mse=1167061888.0\n",
      "Epoch 1000 parameters: m=44623.44140625 b=-97912.4296875 mse=1167061504.0\n",
      "Final model parameters: m=45019.953125 b=-97849.890625 mse=1154176768.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 5. Lower batch size even more to check time-performance tradeoff, expecting poor performance\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 50, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too small batch size converge faster than other without leaving space to improvement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=30205.626953125 b=2158.34619140625 mse=1383410688.0\n",
      "Epoch 200 parameters: m=30582.224609375 b=-262.24005126953125 mse=1371967616.0\n",
      "Epoch 300 parameters: m=30950.0078125 b=-2626.20361328125 mse=1361066880.0\n",
      "Epoch 400 parameters: m=31309.193359375 b=-4934.87060546875 mse=1350682112.0\n",
      "Epoch 500 parameters: m=31659.974609375 b=-7189.5263671875 mse=1340790144.0\n",
      "Epoch 600 parameters: m=32002.5546875 b=-9391.4501953125 mse=1331367424.0\n",
      "Epoch 700 parameters: m=32337.111328125 b=-11541.8642578125 mse=1322392704.0\n",
      "Epoch 800 parameters: m=32663.853515625 b=-13641.978515625 mse=1313843968.0\n",
      "Epoch 900 parameters: m=32982.94921875 b=-15692.974609375 mse=1305701504.0\n",
      "Epoch 1000 parameters: m=33294.578125 b=-17695.984375 mse=1297946752.0\n",
      "Final model parameters: m=33288.01171875 b=-17700.830078125 mse=1297881088.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 6. Returning to best performance settings on experiment 4 with a lower learning rate, this would improve the model\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 200, lr = 0.001, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too small learning rate makes it time consuming to optimize the cost function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=30205.626953125 b=2158.34619140625 mse=1383410688.0\n",
      "Epoch 200 parameters: m=30582.224609375 b=-262.24005126953125 mse=1371967616.0\n",
      "Epoch 300 parameters: m=30950.0078125 b=-2626.20361328125 mse=1361066880.0\n",
      "Epoch 400 parameters: m=31309.193359375 b=-4934.87060546875 mse=1350682112.0\n",
      "Epoch 500 parameters: m=31659.974609375 b=-7189.5263671875 mse=1340790144.0\n",
      "Epoch 600 parameters: m=32002.5546875 b=-9391.4501953125 mse=1331367424.0\n",
      "Epoch 700 parameters: m=32337.111328125 b=-11541.8642578125 mse=1322392704.0\n",
      "Epoch 800 parameters: m=32663.853515625 b=-13641.978515625 mse=1313843968.0\n",
      "Epoch 900 parameters: m=32982.94921875 b=-15692.974609375 mse=1305701504.0\n",
      "Epoch 1000 parameters: m=33294.578125 b=-17695.984375 mse=1297946752.0\n",
      "Epoch 1100 parameters: m=33598.91796875 b=-19652.140625 mse=1290561152.0\n",
      "Epoch 1200 parameters: m=33896.13671875 b=-21562.51953125 mse=1283527552.0\n",
      "Epoch 1300 parameters: m=34186.40234375 b=-23428.22265625 mse=1276829184.0\n",
      "Epoch 1400 parameters: m=34469.875 b=-25250.28125 mse=1270450432.0\n",
      "Epoch 1500 parameters: m=34746.7265625 b=-27029.728515625 mse=1264376448.0\n",
      "Epoch 1600 parameters: m=35017.08984375 b=-28767.55078125 mse=1258592768.0\n",
      "Epoch 1700 parameters: m=35281.14453125 b=-30464.724609375 mse=1253085824.0\n",
      "Epoch 1800 parameters: m=35539.01171875 b=-32122.177734375 mse=1247842432.0\n",
      "Epoch 1900 parameters: m=35790.8515625 b=-33740.88671875 mse=1242850176.0\n",
      "Epoch 2000 parameters: m=36036.80078125 b=-35321.71484375 mse=1238097664.0\n",
      "Final model parameters: m=36030.4921875 b=-35325.6640625 mse=1238043264.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 7. Lower learning rate was too slowly decaying, trying more epochs to check if it's going down\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 2000, batch_size = 200, lr = 0.001, kprint = 100)"
   ]
  },
  {
   "source": [
    "Definitely going down, but taking too much time in comparison with other models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=nan b=nan mse=nan\n",
      "Epoch 200 parameters: m=nan b=nan mse=nan\n",
      "Epoch 300 parameters: m=nan b=nan mse=nan\n",
      "Epoch 400 parameters: m=nan b=nan mse=nan\n",
      "Epoch 500 parameters: m=nan b=nan mse=nan\n",
      "Epoch 600 parameters: m=nan b=nan mse=nan\n",
      "Epoch 700 parameters: m=nan b=nan mse=nan\n",
      "Epoch 800 parameters: m=nan b=nan mse=nan\n",
      "Epoch 900 parameters: m=nan b=nan mse=nan\n",
      "Epoch 1000 parameters: m=nan b=nan mse=nan\n",
      "Final model parameters: m=nan b=nan mse=nan\n"
     ]
    }
   ],
   "source": [
    "#Experiment 8. Lower learning rate is too slow, changing to best params (experiment 4) but increasing learning rate expect better performance\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 200, lr = 0.1, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too high learning rate crash the model, it may be related to divergence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 100 parameters: m=32346.158203125 b=-9998.3603515625 mse=1331691264.0\n",
      "Epoch 200 parameters: m=34326.375 b=-22744.3515625 mse=1282051968.0\n",
      "Epoch 300 parameters: m=36046.53125 b=-33816.4296875 mse=1245328512.0\n",
      "Epoch 400 parameters: m=37540.78125 b=-43434.42578125 mse=1218254976.0\n",
      "Epoch 500 parameters: m=38838.79296875 b=-51789.3046875 mse=1198379520.0\n",
      "Epoch 600 parameters: m=39966.33984375 b=-59046.94140625 mse=1183862912.0\n",
      "Epoch 700 parameters: m=40945.8125 b=-65351.46875 mse=1173326848.0\n",
      "Epoch 800 parameters: m=41796.640625 b=-70827.9765625 mse=1165739520.0\n",
      "Epoch 900 parameters: m=42535.73046875 b=-75585.25 mse=1160329216.0\n",
      "Epoch 1000 parameters: m=43177.7734375 b=-79717.8515625 mse=1156520704.0\n",
      "Final model parameters: m=43029.7734375 b=-79749.0546875 mse=1154690304.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 9. Slightly increasing mini batch size to improve performance from experiment 4\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 300, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "Batch size should not increase any farther, it's value seems optimal for the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 200 parameters: m=36031.0546875 b=-35391.578125 mse=1237790976.0\n",
      "Epoch 400 parameters: m=39927.4609375 b=-60309.42578125 mse=1179094144.0\n",
      "Epoch 600 parameters: m=42353.6484375 b=-75825.1015625 mse=1157760128.0\n",
      "Epoch 800 parameters: m=43864.35546875 b=-85486.2109375 mse=1150376064.0\n",
      "Epoch 1000 parameters: m=44805.015625 b=-91501.796875 mse=1148065536.0\n",
      "Epoch 1200 parameters: m=45390.74609375 b=-95247.609375 mse=1147513088.0\n",
      "Epoch 1400 parameters: m=45755.4609375 b=-97579.984375 mse=1147513088.0\n",
      "Epoch 1600 parameters: m=45982.55078125 b=-99032.234375 mse=1147646720.0\n",
      "Epoch 1800 parameters: m=46123.94921875 b=-99936.484375 mse=1147781376.0\n",
      "Epoch 2000 parameters: m=46212.0 b=-100499.578125 mse=1147885312.0\n",
      "Final model parameters: m=46144.9140625 b=-100508.0859375 mse=1147519744.0\n"
     ]
    }
   ],
   "source": [
    "#Experiment 10. Minibatch size set, increasing epochs to tune best model and check if improvement is made, expecting lower improvement\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 2000, batch_size = 200, lr = 0.01, kprint = 200)"
   ]
  },
  {
   "source": [
    "Converge was reach around 1000 epochs, so a higher epoch is not needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Models results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"imgs/ModelsOutput.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Conclusions\n",
    "\n",
    "- Hyper parameter tunning makes a big difference in model performance. As shown in the graph **experiment number 4** was the best in time-performance evaluation. \n",
    "- Experiment 4 learning rate was small enough to allow a fast optimization of the cost function (MSE) without taking too much time and not allowing for divergence. \n",
    "- Epochs do help optimizing but we should check for the point where convergence starts to avoid training more than needed\n",
    "- Mini batch size has a sweet point between small and high, a too high batch size is not so useful, and a lower one makes training longer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}