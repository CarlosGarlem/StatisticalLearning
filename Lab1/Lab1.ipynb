{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0cd92519e450c25804136c2008eef24c3d70be2e96d599be5f96d22ea88f45fe2",
   "display_name": "Python 3.9.4 64-bit ('pydsEnv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**Lab1**\n",
    "Carlos Garc√≠a 21000475"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  print(\"Enabled compatitility to tf1.x\")"
   ]
  },
  {
   "source": [
    "### Getting the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('inputs/proyecto_training_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "n_train = int(np.ceil(data.shape[0]*0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data[:n_train, :], data[n_train:, :] \n",
    "assert (train.shape[0] + test.shape[0]) == data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pd.DataFrame(train, columns = ['SalePrice', 'OverallQual', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt', 'LotFrontage'])\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'SalePrice'\n",
    "regressor = 'OverallQual'"
   ]
  },
  {
   "source": [
    "### Defining the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(y_real,y_aprox):\n",
    "  return 1/2 * tf.reduce_mean(tf.math.square(y_real - y_aprox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mini batch model\n",
    "def trainModel(x, y, epochs = 100, batch_size = 10, lr = 0.001, kprint = 10):\n",
    "    \n",
    "    #define iterations\n",
    "    total_iterations = x.shape[0] // batch_size\n",
    "\n",
    "    #initializing the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    #initializing data\n",
    "    tensor_x = tf.placeholder(tf.float32, [None, 2], \"tensor_x\")\n",
    "    tensor_y = tf.placeholder(tf.float32, [None, 1], \"tensor_y\")\n",
    "    w = tf.get_variable(\"parameters_m_b\", dtype=tf.float32, shape=[2,1],\n",
    "                    initializer=tf.zeros_initializer())\n",
    "\n",
    "    #estimating values\n",
    "    yhat = tf.matmul(tensor_x, w, name = 'yhat')\n",
    "\n",
    "    #Cost/Error calculation\n",
    "    with tf.name_scope(\"cost_definition\"):\n",
    "        cost = error(tensor_y, yhat)\n",
    "        \n",
    "    #Scalar summary\n",
    "    cost_summary = tf.summary.scalar(name = 'MSE', tensor = cost)\n",
    "\n",
    "    #gradients and cost/error optimization\n",
    "    with tf.name_scope(\"params_update\"):\n",
    "        gradients = tf.gradients(cost, [w], name = 'gradients') #calculating error and gradients\n",
    "        w_update = tf.assign(w, w - lr * gradients[0], name = 'weigths_update') #updating parameters weights\n",
    "    \n",
    "    #with tf.train.MonitoredSession() as session: #this object doesn't generate a clean graph due to initialized variables\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        #Reshaping the data\n",
    "        x = x[:, np.newaxis]\n",
    "        x = np.hstack((x, np.ones_like(x)))\n",
    "        y = y[:, np.newaxis]\n",
    "\n",
    "        feed_dict_model = {tensor_x: x, tensor_y: y} #whole batch dictionary\n",
    "\n",
    "        #Define tensorboard writer and config string\n",
    "        dt_string = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        writer = tf.summary.FileWriter('./graphs/{}_lm_epochs={}_mbatch={}_lr={}'.format(dt_string, epochs, batch_size, lr), session.graph)\n",
    "        \n",
    "        for epoch in range(0, epochs):    \n",
    "            for i in range(0, total_iterations):\n",
    "                start_sample = i * batch_size\n",
    "                end_sample = start_sample + batch_size\n",
    "                x_mb = x[start_sample:end_sample]\n",
    "                y_mb = y[start_sample:end_sample]\n",
    "                \n",
    "                feed_dict = {tensor_x: x_mb, tensor_y: y_mb}\n",
    "                _, weights = session.run([w_update, w], feed_dict = feed_dict) #be careful not to use the same name of a previous variable\n",
    "                #m, b = weights[0, 0], weights[1, 0]\n",
    "\n",
    "                #print(\"Epoch {} iteration {} m={} b={}\".format(epoch,i,m,b))\n",
    "\n",
    "            predictions = session.run(yhat, feed_dict = feed_dict_model)\n",
    "            mse, csummary = session.run([cost, cost_summary], feed_dict = feed_dict_model)\n",
    "            writer.add_summary(csummary, epoch + 1)\n",
    "\n",
    "            if (epoch + 1) % kprint == 0:            \n",
    "                m, b = weights[0, 0], weights[1, 0]\n",
    "                print(\"Epoch {} parameters: m={} b={} mse={}\".format(epoch + 1, m, b, mse))\n",
    "\n",
    "        _, weights = session.run([w_update, w], feed_dict = feed_dict_model)\n",
    "        predictions, mse = session.run([yhat, cost], feed_dict = feed_dict_model)\n",
    "        m, b = weights[0, 0], weights[1, 0]\n",
    "        print(\"Final model parameters: m={} b={} mse={}\".format(m,b,mse))\n",
    "\n",
    "        writer.close()\n",
    "            "
   ]
  },
  {
   "source": [
    "### Graph definition\n",
    "\n",
    "<img src=\"imgs/graph_definition.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Experiment1. Initial experiment, expecting MSE going down. This experiment will work as a starting point to tune next experiments. Small lr and epochs\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 100, batch_size = 250, lr = 0.01, kprint = 10 )"
   ]
  },
  {
   "source": [
    "Good starting point, MSE is going down at a good peace but too few epochs implemented"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment2. Since the MSE decayed really quickly will only add more epochs. This will keep lowering it down\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 250, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "As expected initial hyper parameters work fine, increasing epochs shows a good progress"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3. Experiment with a higher batch size may cause a better performance, so batch_size is increasing to 500\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 500, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too high batch size is not working since MSE is going up again"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 4. Higher batchsize didn't work out, lowering it down a little more to check for optimal hyper parameters, expect better performance\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 200, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "Lowering batch size a bit more makes an improvement without consuming too much additional time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5. Lower batch size even more to check time-performance tradeoff, expecting poor performance\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 50, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too small batch size converge faster than other without leaving space to improvement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6. Returning to best performance settings on experiment 4 with a lower learning rate, this would improve the model\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 200, lr = 0.001, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too small learning rate makes it time consuming to optimize the cost function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 7. Lower learning rate was too slowly decaying, trying more epochs to check if it's going down\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 2000, batch_size = 200, lr = 0.001, kprint = 100)"
   ]
  },
  {
   "source": [
    "Definitely going down, but taking too much time in comparison with other models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 8. Lower learning rate is too slow, changing to best params (experiment 4) but increasing learning rate expect better performance\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 200, lr = 0.1, kprint = 100)"
   ]
  },
  {
   "source": [
    "A too high learning rate crash the model, it may be related to divergence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 9. Slightly increasing mini batch size to improve performance from experiment 4\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 1000, batch_size = 300, lr = 0.01, kprint = 100)"
   ]
  },
  {
   "source": [
    "Batch size should not increase any farther, it's value seems optimal for the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 10. Minibatch size set, increasing epochs to tune best model and check if improvement is made, expecting lower improvement\n",
    "trainModel(ds_train[regressor].values, ds_train[response].values, epochs = 2000, batch_size = 200, lr = 0.01, kprint = 200)"
   ]
  },
  {
   "source": [
    "Converge was reach around 1000 epochs, so a higher epoch is not needed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Models results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"imgs/ModelsOutput.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Conclusions\n",
    "\n",
    "- Hyper parameter tunning makes a big difference in model performance. As shown in the graph **experiment number 4** (epochs = 1000, batch_size = 200, lr = 0.01) was the best in time-performance evaluation. \n",
    "- Experiment 4 learning rate was small enough to allow a fast optimization of the cost function (MSE) without taking too much time and not allowing for divergence. \n",
    "- Epochs do help optimizing but we should check for the point where convergence starts to avoid training more than needed\n",
    "- Mini batch size has a sweet point between small and high, a too high batch size is not so useful, and a lower one makes training longer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}